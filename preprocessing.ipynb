{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Primary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# text processing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "COLORS = ['r', 'g', 'b', 'y']\n",
    "COLUMNS = ['id', 'company', 'sentiment', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       company sentiment                                              tweet\n",
       " 0  Borderlands  Positive  im getting on borderlands and i will murder yo...\n",
       " 1  Borderlands  Positive  I am coming to the borders and I will kill you...\n",
       " 2  Borderlands  Positive  im getting on borderlands and i will kill you ...\n",
       " 3  Borderlands  Positive  im coming on borderlands and i will murder you...\n",
       " 4  Borderlands  Positive  im getting on borderlands 2 and i will murder ...,\n",
       " (74682, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"./Dataset/twitter_training.csv\", names=COLUMNS, header=None)\n",
    "X_test = pd.read_csv(\"./Dataset/twitter_validation.csv\", names=COLUMNS, header=None)\n",
    "\n",
    "# drop \"id\"\n",
    "X_train.drop('id', axis = 1, inplace = True)\n",
    "X_test.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "X_train.head(), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess_tweets:\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    \n",
    "    def remove_null(self):\n",
    "        for col in self.X.columns:\n",
    "            if(self.X[col].isnull().sum() > 0):\n",
    "                if(col == 'company'):\n",
    "                    self.X[col].fillna('Others', inplace=True)\n",
    "                else:\n",
    "                    self.X.dropna(inplace=True)\n",
    "    \n",
    "    def remove_user_mentions(self):\n",
    "        no_user_mentions = []\n",
    "        cleaned_tweets = []\n",
    "        for index in range(self.X.shape[0]):\n",
    "            tweet = self.X['tweet'].iloc[index]\n",
    "            no_user_mentions.append(re.sub(r'@[A-Za-z0-9]+', '', tweet))\n",
    "            cleaned_tweets.append(re.sub('#', '', no_user_mentions[index]))\n",
    "\n",
    "        self.X['cleaned_tweets'] = np.array(cleaned_tweets)\n",
    "        self.X.drop('tweet', axis = 1, inplace = True)\n",
    "    \n",
    "    def convert_text(self):\n",
    "        cleaned_tweets = []\n",
    "        for index in range(self.X.shape[0]):\n",
    "            tweet = self.X['cleaned_tweets'].iloc[index]\n",
    "            \n",
    "            # remove numbers (if any)\n",
    "            tweet = re.sub(r'[^a-z A-Z]', '', tweet)\n",
    "            \n",
    "            # convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "        \n",
    "            cleaned_tweets.append(tweet)\n",
    "\n",
    "        self.X['cleaned_tweets'] = np.array(cleaned_tweets)\n",
    "\n",
    "    def unique_words(self):\n",
    "        unique_words = []\n",
    "        for index in range(self.X.shape[0]):\n",
    "            dict = {}\n",
    "            temp = []\n",
    "            tweet = self.X['cleaned_tweets'].iloc[index]\n",
    "\n",
    "            word_tokens = tweet.split()\n",
    "\n",
    "            for word in word_tokens:\n",
    "                if word not in dict:\n",
    "                    temp.append(word)\n",
    "                    dict.update({word: 1})\n",
    "                else:\n",
    "                    dict[word] += 1\n",
    "            \n",
    "            unique_words.append(\" \".join(temp).strip())\n",
    "        \n",
    "        self.X['cleaned_tweets'] = np.array(unique_words)\n",
    "\n",
    "    def zero_len_remove_null(self):\n",
    "        word_count = []\n",
    "        outlier_tweets = []\n",
    "        zero_length_count = 0\n",
    "        for index in range(self.X.shape[0]):\n",
    "            word_count.append(len(self.X['cleaned_tweets'].iloc[index].split()))\n",
    "            if(word_count[index] == 0):\n",
    "                zero_length_count += 1\n",
    "                self.X['cleaned_tweets'].iloc[index] = np.nan\n",
    "            if(word_count[index] > 30):\n",
    "                outlier_tweets.append((self.X['sentiment'].iloc[index], self.X['cleaned_tweets'].iloc[index]))\n",
    "        self.X.dropna(inplace=True)\n",
    "        return self.X, outlier_tweets, word_count\n",
    "    \n",
    "    def stem_and_lemmatize(self):\n",
    "        cleaned_tweets = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        ps = PorterStemmer()\n",
    "        for index in range(self.X.shape[0]):\n",
    "            tweet = self.X['cleaned_tweets'].iloc[index]\n",
    "\n",
    "            # tokenize tweet\n",
    "            word_tokens = tweet.split()\n",
    "\n",
    "            # first stem and then lemmatize\n",
    "            for count, word in enumerate(word_tokens):\n",
    "                temp = ps.stem(word)\n",
    "                word_tokens[count] = lemmatizer.lemmatize(temp)\n",
    "\n",
    "            # rejoin to form sentence\n",
    "            filtered_sentence = \" \".join(word_tokens).strip()\n",
    "            cleaned_tweets.append(filtered_sentence)\n",
    "\n",
    "        self.X['cleaned_tweets'] = np.array(cleaned_tweets)\n",
    "        # return self.X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess training dataset\n",
    "process_train = Preprocess_tweets(X_train)\n",
    "process_train.remove_null()\n",
    "process_train.remove_user_mentions()\n",
    "process_train.convert_text()\n",
    "process_train.unique_words()\n",
    "process_train.stem_and_lemmatize()\n",
    "X_train, outlier_tweets, word_count = process_train.zero_len_remove_null()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "positive_df = X_train[X_train['sentiment'] == \"Positive\"]\n",
    "negative_df = X_train[X_train['sentiment'] == \"Negative\"]\n",
    "neutral_df = X_train[X_train['sentiment'] == 'Neutral']\n",
    "irrelevant_df = X_train[X_train['sentiment'] == 'Irrelevant']\n",
    "\n",
    "global_pos = \"\"\n",
    "global_neg = \"\"\n",
    "global_neu = \"\"\n",
    "global_irr = \"\"\n",
    "\n",
    "for tweet in positive_df['cleaned_tweets']:\n",
    "    global_pos += tweet\n",
    "    global_pos += ' '\n",
    "\n",
    "for tweet in negative_df['cleaned_tweets']:\n",
    "    global_neg += tweet\n",
    "    global_neg += \" \"\n",
    "\n",
    "for tweet in neutral_df['cleaned_tweets']:\n",
    "    global_neu += tweet\n",
    "    global_neu += \" \"\n",
    "\n",
    "for tweet in irrelevant_df['cleaned_tweets']:\n",
    "    global_irr += tweet\n",
    "    global_irr += \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test dataset\n",
    "process_test = Preprocess_tweets(X_test)\n",
    "process_test.remove_null()\n",
    "process_test.remove_user_mentions()\n",
    "process_test.convert_text()\n",
    "process_test.unique_words()\n",
    "process_test.stem_and_lemmatize()\n",
    "X_test, outlier_tweets_test, word_count_test = process_test.zero_len_remove_null()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 7))\n",
    "\n",
    "sns.boxplot(np.array(word_count))\n",
    "plt.title(\"Word Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_arr = [\n",
    "    {\"tweets\": global_pos, \"sentiment\": \"Positive\"},\n",
    "    {\"tweets\": global_neg, \"sentiment\": \"Negative\"},\n",
    "    {\"tweets\": global_neu, \"sentiment\": \"Neutral\"},\n",
    "    {\"tweets\": global_irr, \"sentiment\": \"Irrelevant\"}\n",
    "]\n",
    "\n",
    "X_train = pd.DataFrame(temp_arr)\n",
    "X_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./Dataset/cleaned_training.csv\")\n",
    "X_test.to_csv('./Dataset/cleaned_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
