{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/sameep/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to /home/sameep/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to /home/sameep/nltk_data...\n",
      "[nltk_data]   Package omw-1.4 is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Primary Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "\n",
    "# text processing libraries\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer, PorterStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# global variables\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "COLORS = ['r', 'g', 'b', 'y']\n",
    "COLUMNS = ['id', 'company', 'sentiment', 'tweet']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(       company sentiment                                              tweet\n",
       " 0  Borderlands  Positive  im getting on borderlands and i will murder yo...\n",
       " 1  Borderlands  Positive  I am coming to the borders and I will kill you...\n",
       " 2  Borderlands  Positive  im getting on borderlands and i will kill you ...\n",
       " 3  Borderlands  Positive  im coming on borderlands and i will murder you...\n",
       " 4  Borderlands  Positive  im getting on borderlands 2 and i will murder ...,\n",
       " (74682, 3))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train = pd.read_csv(\"./Dataset/twitter_training.csv\", names=COLUMNS, header=None)\n",
    "X_test = pd.read_csv(\"./Dataset/twitter_validation.csv\", names=COLUMNS, header=None)\n",
    "\n",
    "# drop \"id\"\n",
    "X_train.drop('id', axis = 1, inplace = True)\n",
    "X_test.drop('id', axis = 1, inplace = True)\n",
    "\n",
    "X_train.head(), X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Preprocess_tweets:\n",
    "    def __init__(self, X):\n",
    "        self.X = X\n",
    "    \n",
    "    def remove_null(self):\n",
    "        for col in self.X.columns:\n",
    "            if(self.X[col].isnull().sum() > 0):\n",
    "                if(col == 'company'):\n",
    "                    self.X[col].fillna('Others', inplace=True)\n",
    "                else:\n",
    "                    self.X.dropna(inplace=True)\n",
    "    \n",
    "    def remove_user_mentions(self):\n",
    "        no_user_mentions = []\n",
    "        cleaned_tweets = []\n",
    "        for index in range(self.X.shape[0]):\n",
    "            tweet = self.X['tweet'].iloc[index]\n",
    "            no_user_mentions.append(re.sub(r'@[A-Za-z0-9]+', '', tweet))\n",
    "            cleaned_tweets.append(re.sub('#', '', no_user_mentions[index]))\n",
    "\n",
    "        self.X['cleaned_tweets'] = np.array(cleaned_tweets)\n",
    "        self.X.drop('tweet', axis = 1, inplace = True)\n",
    "    \n",
    "    def convert_text(self):\n",
    "        cleaned_tweets = []\n",
    "        for index in range(self.X.shape[0]):\n",
    "            tweet = self.X['cleaned_tweets'].iloc[index]\n",
    "            \n",
    "            # remove numbers (if any)\n",
    "            tweet = re.sub(r'[^a-z A-Z]', '', tweet)\n",
    "            \n",
    "            # convert to lower case\n",
    "            tweet = tweet.lower()\n",
    "        \n",
    "            cleaned_tweets.append(tweet)\n",
    "\n",
    "        self.X['cleaned_tweets'] = np.array(cleaned_tweets)\n",
    "\n",
    "    def unique_words(self):\n",
    "        unique_words = []\n",
    "        for index in range(self.X.shape[0]):\n",
    "            dict = {}\n",
    "            temp = []\n",
    "            tweet = self.X['cleaned_tweets'].iloc[index]\n",
    "\n",
    "            word_tokens = tweet.split()\n",
    "\n",
    "            for word in word_tokens:\n",
    "                if word not in dict:\n",
    "                    temp.append(word)\n",
    "                    dict.update({word: 1})\n",
    "                else:\n",
    "                    dict[word] += 1\n",
    "            \n",
    "            unique_words.append(\" \".join(temp).strip())\n",
    "        \n",
    "        self.X['cleaned_tweets'] = np.array(unique_words)\n",
    "\n",
    "    def zero_len_remove_null(self):\n",
    "        word_count = []\n",
    "        outlier_tweets = []\n",
    "        zero_length_count = 0\n",
    "        for index in range(self.X.shape[0]):\n",
    "            word_count.append(len(self.X['cleaned_tweets'].iloc[index].split()))\n",
    "            if(word_count[index] == 0):\n",
    "                zero_length_count += 1\n",
    "                self.X['cleaned_tweets'].iloc[index] = np.nan\n",
    "            if(word_count[index] > 30):\n",
    "                outlier_tweets.append((self.X['sentiment'].iloc[index], self.X['cleaned_tweets'].iloc[index]))\n",
    "        self.X.dropna(inplace=True)\n",
    "        return self.X, outlier_tweets, word_count\n",
    "    \n",
    "    def stem_and_lemmatize(self):\n",
    "        cleaned_tweets = []\n",
    "        lemmatizer = WordNetLemmatizer()\n",
    "        ps = PorterStemmer()\n",
    "        for index in range(self.X.shape[0]):\n",
    "            tweet = self.X['cleaned_tweets'].iloc[index]\n",
    "\n",
    "            # tokenize tweet\n",
    "            word_tokens = tweet.split()\n",
    "\n",
    "            # first lemmatize and then stem\n",
    "            for count, word in enumerate(word_tokens):\n",
    "                temp = lemmatizer.lemmatize(word_tokens)\n",
    "                word_tokens[count] = ps.stem(temp)\n",
    "\n",
    "            # rejoin to form sentence\n",
    "            filtered_sentence = \" \".join(words_tokens).strip()\n",
    "            cleaned_tweets.append(filtered_sentence)\n",
    "\n",
    "        self.X['cleaned_tweets'] = np.array(cleaned_tweets)\n",
    "        # return self.X\n",
    "    \n",
    "    def merge_class(self):\n",
    "        temp_df = self.X[self.X['sentiment'] == 'Positive']\n",
    "        temp_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "UnboundLocalError",
     "evalue": "local variable 'filtered_sentence' referenced before assignment",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mUnboundLocalError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m process_train\u001b[38;5;241m.\u001b[39mconvert_text()\n\u001b[1;32m      6\u001b[0m process_train\u001b[38;5;241m.\u001b[39munique_words()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mprocess_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstem_and_lemmatize\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m X_train, outlier_tweets, word_count \u001b[38;5;241m=\u001b[39m process_train\u001b[38;5;241m.\u001b[39mzero_len_remove_null()\n\u001b[1;32m      9\u001b[0m X_train\u001b[38;5;241m.\u001b[39mmerge_class()\n",
      "Input \u001b[0;32mIn [4]\u001b[0m, in \u001b[0;36mPreprocess_tweets.stem_and_lemmatize\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     81\u001b[0m word_tokens \u001b[38;5;241m=\u001b[39m tweet\u001b[38;5;241m.\u001b[39msplit()\n\u001b[1;32m     83\u001b[0m \u001b[38;5;66;03m# first stem and then lemmatize\u001b[39;00m\n\u001b[0;32m---> 84\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m count, word \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[43mfiltered_sentence\u001b[49m):\n\u001b[1;32m     85\u001b[0m     temp \u001b[38;5;241m=\u001b[39m ps\u001b[38;5;241m.\u001b[39mstem(word)\n\u001b[1;32m     86\u001b[0m     filtered_sentence[count] \u001b[38;5;241m=\u001b[39m lemmatizer\u001b[38;5;241m.\u001b[39mlemmatize(temp)\n",
      "\u001b[0;31mUnboundLocalError\u001b[0m: local variable 'filtered_sentence' referenced before assignment"
     ]
    }
   ],
   "source": [
    "# preprocess training dataset\n",
    "process_train = Preprocess_tweets(X_train)\n",
    "process_train.remove_null()\n",
    "process_train.remove_user_mentions()\n",
    "process_train.convert_text()\n",
    "process_train.unique_words()\n",
    "process_train.stem_and_lemmatize()\n",
    "X_train, outlier_tweets, word_count = process_train.zero_len_remove_null()\n",
    "X_train.merge_class()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocess test dataset\n",
    "process_test = Preprocess_tweets(X_test)\n",
    "process_test.remove_null()\n",
    "process_test.remove_user_mentions()\n",
    "process_test.convert_text()\n",
    "process_test.unique_words()\n",
    "process_test.stem_and_lemmatize()\n",
    "X_test, outlier_tweets_test, word_count_test = process_test.zero_len_remove_null()\n",
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(30, 7))\n",
    "\n",
    "sns.boxplot(np.array(word_count))\n",
    "plt.title(\"Word Count Distribution\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.to_csv(\"./Dataset/cleaned_training.csv\")\n",
    "X_test.to_csv('./Dataset/cleaned_test.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "vscode": {
   "interpreter": {
    "hash": "916dbcbb3f70747c44a77c7bcd40155683ae19c65e1c03b4aa3499c5328201f1"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
